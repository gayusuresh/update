<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>A comparison of ActiveMQ and Kafka</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/16/comparison-activemq-and-kafka" /><author><name>Michael Thirion</name></author><id>94f2fc35-5b7f-462b-aede-7fde4595f809</id><updated>2023-02-16T07:00:00Z</updated><published>2023-02-16T07:00:00Z</published><summary type="html">&lt;p&gt;Many times, I’ve helped customers choose between Red Hat AMQ Broker and Red Hat AMQ Streams. Red Hat AMQ Broker is a messaging solution based on the upstream Apache ActiveMQ Artemis project. Red Hat AMQ Streams, on the other hand, is a data streaming platform based on the combination of the upstream Apache Kafka and Strimzi projects.&lt;/p&gt; &lt;p&gt;We could perform a thorough feature-to-feature comparison to decide between those two. Instead, I will provide an alternative view from the perspective of the philosophy behind those two initiatives.&lt;/p&gt; &lt;h2&gt;Pushing messages versus pulling events&lt;/h2&gt; &lt;p&gt;Apache ActiveMQ Artemis exposes open interfaces and open protocols. The client applications can exchange information with the server through JMS, AMQP, or MQTT. As a consequence, those client applications can be written in multiple languages such as Java, .NET, Javascript, and Python. So, the client applications send messages to the server, as depicted in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/diag1-min_1.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/diag1-min_1.jpg?itok=DCqqbudO" width="511" height="313" alt="A diagram illustrating client applications sending messages to an ActiveMQ Broker." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1. Client applications send messages to the ActiveMQ broker.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;On the other hand, Apache Kafka uses its own protocol. Only clients using the Kafka API can interact with it. Therefore, here, it's rather the server that will get information from external systems (Figure 2). This opposite approach changes from the concept of messages to the concept of events.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/diag2-min_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/diag2-min_0.jpg?itok=Mq9LzhfV" width="600" height="247" alt="A diagram illustrating the Kafka broker extracting events from systems." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2. The Kafka broker extracts events from systems.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Quality of service&lt;/h2&gt; &lt;p&gt;Considering data as events rather than messages has led to a different way of defining certain responsibilities. Kafka will query data from external systems and store them for later consumption. It will not take any responsibility for how to interpret the data or for determining its importance. This responsibility will be left to the consumers.&lt;/p&gt; &lt;p&gt;ActiveMQ, on the other hand, will be requested by an external system to carry the data for it and will implicitly accept some responsibility for handling the message with a proper quality of service.  &lt;/p&gt; &lt;p&gt;We can understand that difference easily if we consider the exactly-once delivery quality of service. This is achievable with both technologies. But on one hand, it will be guaranteed by the server, while on the other hand, it will be up to the consumer to enforce it. Even though there are some experimental components aimed at bringing the concept of transactions to Kafka, we already know from the CAP theorem that it will have to leave behind either partitioning or high availability.&lt;/p&gt; &lt;p&gt;To go one step further, Kafka will not even ensure the data is safely stored. The data will be sent to the cache, which will eventually flush it to disk. Alternatively, ActiveMQ performs synchronous writing to ensure acknowledged data can never be lost.&lt;/p&gt; &lt;h2&gt;Clustering&lt;/h2&gt; &lt;p&gt;With ActiveMQ, the consumer doesn’t have to be connected exactly where the message is produced. The proper routing of the message is another responsibility taken by the server. It’s the same principle as having postmen to ensure the delivery of the mail to the right addresses.&lt;/p&gt; &lt;p&gt;As a consequence, if the clustering feature can indeed provide an increase in the inbound throughput, it is usually accompanied by a decrease in the outbound latency due to the hops the data need to cross to reach the right consumers over the cluster (Figure 3).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/diag3-min.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/diag3-min.jpg?itok=B7bKWuTU" width="402" height="370" alt="A diagram showing an ActiveMQ cluster distribution across three nodes." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3. ActiveMQ clustering distribution with producers and consumers on different nodes.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;p&gt;In the case of Kafka, the server doesn’t take any responsibility for ensuring that events are sent to the proper destination. Instead, it will help the client applications willing to consume an event to connect to the node where the event is located (Figure 4), which guarantees both maximum throughput and minimum latency. Here, that would be closer to acting as the post office.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/diag4-min.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/diag4-min.jpg?itok=2nj2H91P" width="473" height="384" alt="A diagram showing a Kafka cluster distribution across three nodes, resulting in the producer and the consumer connecting to the same node, where the partition leader is located.." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4. Kafka clustering distribution results in the producer and the consumer being connected to the same node, where the partition leader is located.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Is Kafka better than ActiveMQ?&lt;/h2&gt; &lt;p&gt;It is true that ActiveMQ doesn’t fit with the cloud as well as Kafka does. But it doesn’t mean that there are no longer any use cases for ActiveMQ. The cloud is not only a change of technology but also a change of paradigm. Today, we might still have more use cases for ActiveMQ than for Kafka, as there are still plenty of business use cases requiring guaranteed deliveries of information.  Actually, comparing ActiveMQ to Kafka is not very far from comparing a relational database to a NoSQL one.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/16/comparison-activemq-and-kafka" title="A comparison of ActiveMQ and Kafka"&gt;A comparison of ActiveMQ and Kafka&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Thirion</dc:creator><dc:date>2023-02-16T07:00:00Z</dc:date></entry><entry><title type="html">How to initialize an Array in Java in 4 simple ways</title><link rel="alternate" href="http://www.mastertheboss.com/java/4-ways-to-initialize-an-array-in-java/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/4-ways-to-initialize-an-array-in-java/</id><updated>2023-02-15T07:17:47Z</updated><content type="html">This article discusses about array initialization in Java, showing multiple ways to initialize an array, some of them you probably don’t know! How to declare an array in Java Firstly, some background. Java classifies types as primitive types, user-defined types, and array types. An array type is a region of memory that stores values in ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Apache Maven Faqs for Java Developers</title><link rel="alternate" href="http://www.mastertheboss.com/jboss-frameworks/jboss-maven/apache-maven-faqs/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jboss-frameworks/jboss-maven/apache-maven-faqs/</id><updated>2023-02-15T07:13:02Z</updated><content type="html">Welcome to our tutorial on Apache Maven FAQs! Apache Maven is a popular build automation tool used for Java projects. It helps developers manage project dependencies, build, test, and deploy projects. In this tutorial, we will cover some frequently asked questions about Maven and provide helpful answers and explanations. Overview of Apache Maven Apache Maven ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Build smaller container images using S2I</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/15/build-smaller-container-images-using-s2i" /><author><name>Lumír Balhar</name></author><id>4abf9445-dfd5-4627-9d7b-222d91f1a547</id><updated>2023-02-15T07:00:00Z</updated><published>2023-02-15T07:00:00Z</published><summary type="html">&lt;p&gt;The Source-to-Image (S2I) framework makes it easy for developers to transfer their projects into ready-to-use and reproducible &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images. S2I consists of a &lt;a href="https://github.com/openshift/source-to-image"&gt;tool&lt;/a&gt; that prepares your container, along with many &lt;a href="https://github.com/sclorg"&gt;base images&lt;/a&gt; to choose from. S2I base container images are ready for multiple database engines like &lt;a href="https://github.com/sclorg/postgresql-container/"&gt;PostgreSQL&lt;/a&gt; or &lt;a href="https://github.com/sclorg/mariadb-container"&gt;MariaDB&lt;/a&gt; as well as for programming language ecosystems like &lt;a href="https://github.com/sclorg/s2i-python-container"&gt;Python&lt;/a&gt;, &lt;a href="https://github.com/sclorg/s2i-php-container"&gt;PHP&lt;/a&gt;, &lt;a href="https://github.com/sclorg/s2i-nodejs-container/"&gt;Node.js&lt;/a&gt;, and more.&lt;/p&gt; &lt;h2&gt;The need for a minimal container image&lt;/h2&gt; &lt;p&gt;The biggest advantage of the S2I base container images is their universality. The base images contain a carefully selected set of pre-installed RPM packages that make it easy to build applications and their dependencies from source. The images also contain scripts that can handle the whole process of preparation of the final container image.&lt;/p&gt; &lt;p&gt;However, this versatility—their biggest advantage—is also their biggest flaw. The size of the base images has grown as we tried to support the majority of use cases reported by developers. The result is that the base container images might be too big for your use case and likely contain some tools and libraries that are not relevant for your application. This is especially problematic given the current popularity of &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;. And so, we have decided to start a new range of minimal container images addressing this market.&lt;/p&gt; &lt;h2&gt;Use cases&lt;/h2&gt; &lt;p&gt;What does this mean? What is the difference? Let's take a look, for example, at the S2I &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; container image with Python 3.9 based on &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux (RHEL)&lt;/a&gt; 8. We started by switching from the UBI (the &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Universal Base Image&lt;/a&gt; based on RHEL) to the UBI-minimal base layer. The main difference here is that UBI-minimal has &lt;code&gt;microdnf&lt;/code&gt; (written in &lt;a href="https://developers.redhat.com/topics/c"&gt;C&lt;/a&gt;) instead of &lt;code&gt;dnf&lt;/code&gt; (written in Python) as the main package manager. This change alone saves us half of the original size (75.5 MB → 37.6 MB).&lt;/p&gt; &lt;p&gt;The rest of the saved disk space comes from the minimized set of packages we pre-install into the minimal Python container image. The final result is that we have managed to shrink the total image size from 891 MB to 201 MB—a full 77% reduction in the size of the full container image.&lt;/p&gt; &lt;p&gt;The situation is similar for the Node.js 16 image, also based on RHEL 8. The minimal container image has only 188 MB instead of the full container image with 638 MB—more than 70 % saved.&lt;/p&gt; &lt;p&gt;Are there any disadvantages of the minimal container images? In general, no. If your project and all its dependencies are all in interpreted languages, the minimal container images will work for you the same way as the original does. A problem might appear if you need to compile some parts of your project or if any of your dependencies are not provided in pre-compiled packages (like &lt;code&gt;wheels&lt;/code&gt; for Python). In that case, you can either use the full image, or you'll need to install devel libraries and header files into the minimal image. That requires some manual work, but do not be afraid; it is rather straightforward.&lt;/p&gt; &lt;h2&gt;2 ways to use the minimal image&lt;/h2&gt; &lt;p&gt;There are two possible ways to get the benefits of a minimal container image even when you need extra devel libraries and header files. The first one is to create a custom Dockerfile and build a custom container image on top of the minimal container image—in this case, you can still use all the universal scripts from the minimal container image. This approach is good if you need something special for both buildtime and runtime, because the installed packages stay in the image.&lt;/p&gt; &lt;p&gt;The second option is to build the application and its dependencies on the full container image and use the minimal one only for runtime. This approach is great if you need something special only during building but not at runtime. This way, all the pre-installed packages from the full container image are available when you need them.&lt;/p&gt; &lt;p&gt;Let's say we need to install &lt;code&gt;python3-devel&lt;/code&gt; and &lt;code&gt;gcc&lt;/code&gt; to be able to compile the &lt;code&gt;uwsgi&lt;/code&gt; package from source. An example of the custom Dockerfile might look like this.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;FROM python-39-minimal # Add application sources to a directory that the assemble script expects them # and set permissions so that the container runs without root access USER 0 ADD app-src /tmp/src RUN /usr/bin/fix-permissions /tmp/src # Install packages necessary for compiling uwsgi from source RUN microdnf install -y gcc python39-devel USER 1001 # Install the dependencies RUN /usr/libexec/s2i/assemble # Set the default command for the resulting image CMD /usr/libexec/s2i/run&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the next example, we use the full container image and all its pre-installed capabilities to build the app's dependencies, and then we move the entire prepared virtual environment to the minimal container image and additionally install &lt;code&gt;httpd&lt;/code&gt;, which is a runtime dependency for our application.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# Part 1 - build FROM python-39 as builder # Add application sources to a directory that the assemble script expects them # and set permissions so that the container runs without root access USER 0 ADD app-src /tmp/src RUN /usr/bin/fix-permissions /tmp/src USER 1001 # Install the application's dependencies from PyPI RUN /usr/libexec/s2i/assemble # Part 2 - deploy FROM python-39-minimal # Copy app sources together with the whole virtual environment from the builder image COPY --from=builder $APP\_ROOT $APP\_ROOT # Install httpd package - runtime dependency of our application USER 0 RUN microdnf install -y httpd USER 1001 # Set the default command for the resulting image CMD /usr/libexec/s2i/run&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you have any questions, want to discuss something with us, or just want to report that the minimal container images work for you, please feel free to open an issue in one of the relevant GitHub repositories mentioned above. We'll be happy to help you and hear about your use cases.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/15/build-smaller-container-images-using-s2i" title="Build smaller container images using S2I"&gt;Build smaller container images using S2I&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Lumír Balhar</dc:creator><dc:date>2023-02-15T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.0.0.Alpha4 released - Fourth iteration of our Jakarta EE 10 stream</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-0-0-alpha4-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-0-0-alpha4-released/</id><updated>2023-02-15T00:00:00Z</updated><published>2023-02-15T00:00:00Z</published><summary type="html">As you might know by now, we started a Quarkus 3.0 effort last year and we are continuing this effort, which was described here, here, here, and here. Quarkus 3.0.0.Alpha4 is the fourth iteration of this work and it marks a huge milestone: the Jakarta EE 10 stream is now...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-02-15T00:00:00Z</dc:date></entry><entry><title>5 things developers should know about edge computing</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/14/5-things-developers-should-know-about-edge-computing" /><author><name>Hugo Guerrero</name></author><id>2b9633ed-7459-4109-acc0-ea8efc1b5cdb</id><updated>2023-02-14T07:00:00Z</updated><published>2023-02-14T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;Edge computing&lt;/a&gt; has become a major part of our current technology ecosystem, from your neighborhood Dunkin’ Donuts to the watch on your wrist. In fact, Gartner estimates that “about 10% of enterprise-generated data is created and processed outside a traditional centralized data center or cloud” (essentially, edge). And those applications appear poised to expand: &lt;a href="https://www.gartner.com/smarterwithgartner/what-edge-computing-means-for-infrastructure-and-operations-leaders/"&gt;according to Gartner&lt;/a&gt;, that number will grow to 75% by 2025.&lt;/p&gt; &lt;p&gt;Still, edge computing remains a niche topic that many people are unfamiliar with. This article highlights five facts for developers to know about edge.&lt;/p&gt; &lt;h2&gt;1. Edge is everywhere&lt;/h2&gt; &lt;p&gt;Here are some examples of different edge computing use cases (also shown in Figure 1):&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="3"&gt;&lt;strong&gt;A device edge: &lt;/strong&gt;Standalone (non-clustered) systems that directly connect sensors via non-Internet protocols—for example, a coffee shop or your local restaurant.&lt;/li&gt; &lt;li aria-level="3"&gt;&lt;strong&gt;End-user premises edge:&lt;/strong&gt; This includes retail stores, trains, or even a house or car.&lt;/li&gt; &lt;li aria-level="3"&gt;&lt;strong&gt;Service provider edge:&lt;/strong&gt; Edge tiers located between the core or regional datacenters and the last mile access are commonly owned and operated by a telco or Internet service provider. &lt;/li&gt; &lt;li aria-level="3"&gt;&lt;strong&gt;Provider or enterprise core: &lt;/strong&gt;Traditional “non-edge” tiers owned and operated by public cloud providers, telco service providers, or large enterprises. &lt;/li&gt; &lt;/ul&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Where is Edge Computing" data-entity-type="file" data-entity-uuid="8606cdc4-53d8-4e62-bff2-a2465713cbbb" height="248" src="https://developers.redhat.com/sites/default/files/inline-images/Hugo%202.png" width="629" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: The meaning of edge and location by use.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;/ul&gt;&lt;p&gt;To see more use cases and how companies are using edge technologies, refer to this &lt;a href="https://www.redhat.com/en/topics/edge-computing/what-is-edge-computing#companies-using-edge-computing"&gt;article&lt;/a&gt; from Red Hat.&lt;/p&gt; &lt;p&gt;Edge, at its simplest, is localized data processing at the place at or near where it’s created. By placing computing services closer to these locations, users benefit from faster, more reliable services, and companies can benefit from the flexibility of &lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-is-hybrid-cloud"&gt;hybrid cloud&lt;/a&gt; computing.&lt;/p&gt; &lt;p&gt;To go more in-depth about what edge computing entails, read this article: &lt;a href="https://www.redhat.com/en/topics/edge-computing/what-is-edge-computing#:~:text=Edge%20computing%20is%20computing%20that,flexibility%20of%20hybrid%20cloud%20computing"&gt;What is edge computing?&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;2. Edge is growing exponentially&lt;/h2&gt; &lt;p&gt;Edge computing is becoming more relevant to the requirements of current organizations, companies, and the public sector trying to develop applications in remote locations and enabling localized computing capabilities where needed. Gartner also insists in their 2022 document &lt;a href="https://www.gartner.com/en/documents/4007176"&gt;The Distributed Enterprise Drives Computing to the Edge&lt;/a&gt; that “enterprises need to make plans to capture the opportunities and prepare for the challenges of managing data in edge environments.”&lt;/p&gt; &lt;p&gt;Locations that can benefit from the edge model include disaster zones where the deployment of servers and computer power is needed to communicate and spread resources or cruise ships that need to provide services. Even the International Space Station needs to run software, process data, and communicate with others. To learn more, see &lt;a href="https://www.redhat.com/en/resources/edge-computing-in-space-brief"&gt;Edge computing in action: Space&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;3. You can leverage your cloud development knowledge for edge&lt;/h2&gt; &lt;p&gt;Currently, edge technology may seem like a niche field, but translating your own knowledge of cloud to edge is easier than you think. Practices that have been developed in the cloud-based space over the last 20 years can be adapted and used for edge computing development.&lt;/p&gt; &lt;p&gt;One example of this includes one of the most commonly known languages, &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt;. While old versions of Java had known limitations, new frameworks like Quarkus and Java Virtual Machines like GraalVM allow existing cloud developers to reuse their current knowledge. Java has consistently been in the top three programming languages on the TIOBE ratings for the past eighteen years. &lt;/p&gt; &lt;p&gt;Other aspects that can be used in both cloud and edge include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps&lt;/a&gt;, which focuses on software-defined life cycles and emphasizes automation and security. Applications can be deployed consistently and with quality. &lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/containers"&gt;Container images&lt;/a&gt;. Developers can easily upgrade and update code on devices running smart advertisements.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;With the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Red Hat build of Quarkus&lt;/a&gt;, you can deploy, create, and develop applications that are serverless friendly and lightweight in resources. Those applications can then be packaged as containers to be run with &lt;a href="https://www.redhat.com/en/topics/containers/what-is-podman"&gt;Podman&lt;/a&gt; and &lt;a href="https://www.redhat.com/en/technologies/device-edge"&gt;Red Hat Device Edge&lt;/a&gt;, or as native builds for &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; for edge.&lt;/p&gt; &lt;p&gt;The benefits of this Quarkus Java framework with edge capabilities (illustrated in Figure 2) include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Reduced memory utilization.&lt;/li&gt; &lt;li aria-level="1"&gt;Fast start-up time, which allows for automatic scaling up and down.&lt;/li&gt; &lt;li aria-level="1"&gt;Smaller disk footprint, which significantly reduces the size of application binaries and container images.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Quarkus Java Framework" data-entity-type="file" data-entity-uuid="d7be45d7-2e5d-43de-b476-5e210f2095f2" height="315" src="https://developers.redhat.com/sites/default/files/inline-images/Hugo%201.png" width="717" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Breaking down the Quarkus Java framework.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;With edge-to-cloud pipelines, you can implement scalable, security-focused &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven data architecture&lt;/a&gt; to allow better visibility and real-time decision-making at the network edge. Companies like Red Hat provide a robust set of solutions to create lightweight applications, process and store data on-site, as well as use business rules and algorithms to lead decision-making at edge sites. &lt;/p&gt; &lt;h2&gt;4. Applications at the edge boost performance and reduce latency&lt;/h2&gt; &lt;p&gt;We frequently host data on centralized platforms, which introduces latency when consumers attempt to access it from distant places. Requesting data from these data centers in unreliable or spotty internet locations can be slow. This problem is resolved by edge computing, which keeps data on edge servers for easy access.&lt;/p&gt; &lt;p&gt;Because data can be retrieved directly from the endpoints rather than from a remote centralized data center, then sent back to the endpoints, organizations may avoid concerns with speed and connectivity thanks to edge computing. Applications can continue to be improved for better performance and a better user experience by cutting down on the distance they have to travel to retrieve data from a data center.&lt;/p&gt; &lt;p&gt;Additionally, keeping data at the edge lowers operating expenses. For enterprises worldwide, using cloud services to host data, especially large volumes of data, is extremely expensive. Organizations spend more money the more data is moved on centralized hosting providers. It becomes crucial to understand what data has to be transported from the production location to these centers.&lt;/p&gt; &lt;p&gt;However, because there is less need to transfer data to the cloud when using edge computing, businesses can pay less on operational expenditures. The amount of bandwidth required to handle the data transfer is also decreased because data is classified, cleaned, and compressed in the same site where it is generated.&lt;/p&gt; &lt;h2&gt;5. Edge enhances privacy protections and data security&lt;/h2&gt; &lt;p&gt;In the world of IT, data security and privacy protection are hot topics. Because data is handled locally rather than coming from centralized servers, edge computing offers a higher level of data security and privacy protection.&lt;/p&gt; &lt;p&gt;This in no way implies, however, that edge computing devices are not at all vulnerable. In no way. It merely implies that there is less data to be processed at the edge, indicating that there isn't a huge amount of data that can be exploited by hackers.&lt;/p&gt; &lt;p&gt;In other words, when data kept on centralized servers are breached, privacy can be readily jeopardized because they have more detailed information about individuals, places, and events. Since edge computing only generates, processes, and analyzes the data that is immediately necessary, other parts of the system can continue to function.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Developers should be aware of these key takeaways about edge computing:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Edge is growing and will continue to evolve in the future.&lt;/li&gt; &lt;li&gt;You can apply your knowledge of the cloud to edge computing concepts.&lt;/li&gt; &lt;li&gt;Edge can benefit companies financially by improving efficiency and data security.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Edge is becoming a must for all in order to be successful. Developers are now in a position to capture the opportunity of edge.  To learn more about how you can work with edge, visit our &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge computing&lt;/a&gt; topic page.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/14/5-things-developers-should-know-about-edge-computing" title="5 things developers should know about edge computing"&gt;5 things developers should know about edge computing&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Hugo Guerrero</dc:creator><dc:date>2023-02-14T07:00:00Z</dc:date></entry><entry><title>How debugging Go programs with Delve and eBPF is faster</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/13/how-debugging-go-programs-delve-and-ebpf-faster" /><author><name>Derek Parker</name></author><id>baa4a706-58dd-40c1-915c-e29f5b85965a</id><updated>2023-02-13T07:00:00Z</updated><published>2023-02-13T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, I will explain how to use &lt;a href="https://github.com/go-delve/delve"&gt;Delve&lt;/a&gt; to trace your Go programs and how Delve leverages eBPF under the hood to maximize efficiency and speed. The goal of Delve is to provide developers with a pleasant and efficient Go debugging experience. In that vein, this article focuses on how we optimized the function tracing subsystem so you can inspect your programs and get to root-cause analysis quicker. Delve has two different backends for its tracing implementation, one is ptrace based, while the other uses eBPF. If you’re unfamiliar with any of these terms, don’t worry, I will explain along the way.&lt;/p&gt; &lt;h2&gt;What is program tracing?&lt;/h2&gt; &lt;p&gt;Tracing is a technique that allows a developer to see what the program is doing during execution. As opposed to typical debugging techniques, this method does not require direct user interaction. One of the most well-known tracing tools is &lt;a href="https://strace.io/"&gt;strace&lt;/a&gt;, which allows developers to see which system calls their program during execution.&lt;/p&gt; &lt;p&gt;While the aforementioned strace tool is useful for gaining insight into system calls, the Delve trace command allows you to gain insight into what is happening in "userspace" within your Go programs. This Delve trace technique allows you to trace arbitrary functions in your program in order to see the inputs and outputs of those functions. Additionally, you can also use this tool to gain insight into the control flow of your program without the overhead of an interactive debugging session as it will also display with Goroutine is executing the function. For highly concurrent programs this can be a quicker way to gain insights into your programs execution without starting a full interactive debugging session.&lt;/p&gt; &lt;h2&gt;How to trace Go programs with Delve&lt;/h2&gt; &lt;p&gt;Delve allows you to trace your Go programs by invoking the &lt;code&gt;dlv trace&lt;/code&gt; subcommand. The subcommand accepts a regular expression and will execute your program, setting a tracepoint on each function that matches the regular expression and displaying the results in real time.&lt;/p&gt; &lt;p&gt;The following program is an example:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;package main import "fmt" func foo(x, y int) (z int) {         fmt.Printf("x=%d, y=%d, z=%d\n", x, y, z)         z = x + y         return } func main() {         x := 99         y := x * x         z := foo(x, y)         fmt.Printf("z=%d\n", z) }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tracing this program will give you the following output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ dlv trace foo &gt; goroutine(1): main.foo(99, 9801) x=99, y=9801, z=0 &gt;&gt; goroutine(1): =&gt; (9900) z=9900 Process 583475 has exited with status 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see, we supplied &lt;code&gt;foo&lt;/code&gt; as the regexp, which in this case, matched the function of the same name in the main package. The output prefixed with &lt;code&gt;&gt;&lt;/code&gt; denotes the function being called and shows the arguments the function was called by, while the output prefixed with &lt;code&gt;&gt;&gt;&lt;/code&gt; denotes the return from the function and the return value associated with it. All input and output lines are prefixed with the Goroutine executing at the time.&lt;/p&gt; &lt;p&gt;By default, the &lt;code&gt;dlv trace&lt;/code&gt; command uses the ptrace based backend, however adding the &lt;code&gt;--ebpf&lt;/code&gt; flag will enable the experimental eBPF based backend. Using the previous example, if we were to invoke the trace subcommand like the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ dlv trace –ebpf foo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We would receive similar output. However, what happens behind the scenes is much different and significantly more efficient.&lt;/p&gt; &lt;h2&gt;The inefficiencies of ptrace&lt;/h2&gt; &lt;p&gt;By default, Delve will use the ptrace syscall in order to implement the tracing feature. The &lt;a href="https://man7.org/linux/man-pages/man2/ptrace.2.html"&gt;ptrace&lt;/a&gt; is a syscall that allows programs to observe and manipulate other programs on the same machine. In fact, on Unix systems, Delve uses this ptrace functionality to implement many low-level functionalities provided by the debugger, such as reading/writing memory, controlling execution, and more.&lt;/p&gt; &lt;p&gt;While ptrace is a useful and powerful mechanism, it suffers from inherent inefficiencies. First, the fact that ptrace is a syscall means that we must cross the user space/kernel space boundary, which adds overhead every time the function is used. This is compounded by the number of times we have to invoke ptrace in order to achieve the desired results. Considering the previous example, the following is a rough outline of the tracing implementation steps using ptrace:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Start the program and attach the debugger using `ptrace(PT_ATTACH)`.&lt;/li&gt; &lt;li&gt;Set a breakpoint at each function which matches the provided regular expression, using `ptrace` to insert the breakpoint instruction into the traced processes executable memory.&lt;/li&gt; &lt;li&gt;Additionally, set breakpoint at each return instruction for that function.&lt;/li&gt; &lt;li&gt;Continue the program, again using `ptrace(PT_CONT)`.&lt;/li&gt; &lt;li&gt;Hit breakpoint at function entry, and read function arguments. This step can involve many ptrace calls as we read CPU registers, memory on the stack and memory in the heap if we must dereference a pointer.&lt;/li&gt; &lt;li&gt;Continue the program again using `ptrace(PT_CONT)`.&lt;/li&gt; &lt;li&gt;Hit breakpoint at function return, going through the same aforementioned process to read variables potentially involving many more calls to `ptrace` to read registers and memory.&lt;/li&gt; &lt;li&gt;Continue the program again using `ptrace(PT_CONT)`.&lt;/li&gt; &lt;li&gt;Repeat until the program ends.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Obviously, the more arguments and return values the function has, the more expensive every stop becomes. All the time the debugger spends making these `ptrace` syscalls, the program we are tracing is paused and not executing any instructions. From the users’ perspective, this makes the program run significantly slower than it otherwise would. Now, for development and debugging, maybe this isn’t such a big deal, but time is precious, and we should endeavor to do things as quickly as possible. The quicker your program runs while tracing, the quicker you can get to the root cause of the issue you’re trying to debug.&lt;/p&gt; &lt;p&gt;Now, the question becomes, how can we make this better? In the next section, we discuss the new eBPF based backend and how it improves upon this approach.&lt;/p&gt; &lt;h2&gt;How eBPF is faster than ptrace&lt;/h2&gt; &lt;p&gt;One of the biggest speed and efficiency improvements we can make is to avoid a lot of the syscall overhead altogether. This is where &lt;a href="https://ebpf.io/"&gt;eBPF&lt;/a&gt; comes into play because instead of setting breakpoints on each function, we can instead set uprobes on function entry and exit and attach small &lt;a href="https://github.com/go-delve/delve/blob/master/pkg/proc/internal/ebpf/bpf/trace.bpf.c"&gt;eBPF programs&lt;/a&gt; to them. Delve uses the &lt;a href="https://github.com/cilium/ebpf"&gt;Cilium eBPF&lt;/a&gt; Go library to &lt;a href="https://github.com/go-delve/delve/blob/master/pkg/proc/internal/ebpf/helpers.go#L105"&gt;load and interact&lt;/a&gt; with the eBPF programs.&lt;/p&gt; &lt;p&gt;Each time the probe is hit, the kernel will invoke our eBPF program and then continue the main program once it has completed. The small eBPF program we write will handle all of the steps listed above at function entry and exit but without all the syscall context switching because the program executes directly within kernel space. Our eBPF program is able to communicate with the debugger in userspace via eBPF ringbuffer and map data structures, allowing Delve to collect all of the information it needs.&lt;/p&gt; &lt;p&gt;The benefit of this approach is that the time the program we are tracing needs to be paused is significantly decreased. Running our eBPF program when a probe is hit is much quicker than invoking multiple syscalls at function entry and exit.&lt;/p&gt; &lt;h2&gt;The flow of tracing and debugging using eBPF&lt;/h2&gt; &lt;p&gt;Again, using the previous example, the following is a rough outline of the tracing implementation steps using eBPF:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Start the program and attach using `ptrace(PT_ATTACH)`.&lt;/li&gt; &lt;li&gt;Load all uprobes into the kernel for each function to trace.&lt;/li&gt; &lt;li&gt;Continue the program using `ptrace(PT_CONT)`.&lt;/li&gt; &lt;li&gt;Hit uprobes at function entry / exit. In kernel space, each time a probe is hit, the kernel runs our eBPF program, which gathers function arguments or return values and sends them back to userspace. In user space, read from eBPF ringbuffer as function arguments, and return values are sent.&lt;/li&gt; &lt;li&gt;Repeat until the program ends.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Using this method, Delve is able to trace a program in significantly less time than with the default ptrace implementation. Now, you may ask, if it is so much more efficient to use this method, why not make it the default? Eventually, it likely will be made default. But for the time being, development is still ongoing to improve this eBPF based backend and ensure it has parity with the ptrace based one. However, you can still use it today by supplying the `--ebpf` flag during the `dlv trace` invocation.&lt;/p&gt; &lt;p&gt;To give a sense of how much more efficient this method is, I measured a different example program running by itself and then under the different tracing methods with the following results.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Program execution: 23.7µs With eBPF trace: 683.1µs With ptrace tracing: 2.3s&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The numbers speak for themselves!&lt;/p&gt; &lt;h2&gt;Why not use uretprobes?&lt;/h2&gt; &lt;p&gt;If you're familiar with eBPF a uprobes / uretprobes you may be asking yourself why we use uprobes for everything as opposed to simply using uretprobes to capture return arguments. The explanation for this gets relatively complex, but the short version is that the Go runtime needs to inspect the call stack at various times during the execution of a Go program. When uretprobes are attached to a function they overwrite the return address of that function on the stack. When the Go runtime then inspects the stack it finds an unexpected return address for the function and will end up fatally exiting the program. To work around this we simply use uprobes and leveraging Delves ability to inspect the machine instructions of the program to set probes at each return instruction for a function.&lt;/p&gt; &lt;h2&gt;Delve debugs Go code faster with eBPF&lt;/h2&gt; &lt;p&gt;The overall goal of Delve is to help developers find bugs in their Go code as quickly as possible. To do this, we leverage the latest methods and tech available and try to push the boundaries of what a debugger can accomplish. Delve leverages eBPF under the hood to maximize efficiency and speed. User space tracing is a great tool for any engineer to have in their toolbox, and we aim to make it efficient and easy to use.&lt;/p&gt; &lt;p&gt;Building and delivering modern, innovative apps and services is more complicated and fast-moving than ever. Join the Red Hat Developer program for tools, technologies, and community to level up your knowledge and career. &lt;a href="https://developers.redhat.com/about"&gt;Learn more...&lt;/a&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/13/how-debugging-go-programs-delve-and-ebpf-faster" title="How debugging Go programs with Delve and eBPF is faster"&gt;How debugging Go programs with Delve and eBPF is faster&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Derek Parker</dc:creator><dc:date>2023-02-13T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.33.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/02/kogito-1-33-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/02/kogito-1-33-0-released.html</id><updated>2023-02-13T00:58:30Z</updated><content type="html">We are glad to announce that the Kogito 1.33.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Quarkus 2.16 integration * Infinispan upgrade to version 14.0.4 * Keycloack upgrade to version 20.0.2 * New Data Index addons that allow running indexing capabilities as part of Kogito runtimes. For more details, visit the complete .  This includes the following new addons as Quarkus extensions: * kogito-addons-quarkus-data-index-infinispan * kogito-addons-quarkus-data-index-mongodb * kogito-addons-quarkus-data-index-inmemory * Kogito-addons-quarkus-data-index-postgresql * Integration with AsyncAPI quarkiverse extension * Flyway added to help migrate Data Index schema based on Oracle database with Kogito upgrade. * Serverless Workflow can now integrate with Camel Routes defined within the same Maven project. See this blog post for more information.  For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.26.0 artifacts are available at the . A detailed changelog for 1.33.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Quarkus Newsletter #29 - February</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-newsletter-29/" /><author><name>James Cobb</name></author><id>https://quarkus.io/blog/quarkus-newsletter-29/</id><updated>2023-02-13T00:00:00Z</updated><content type="html">Read the February newletter to get the latest articles, blogs and insights on Quarkus. Read "Deploy serverless Java apps with Quarkus on Azure Functions" to learn about Quarkus Funqy and its built-in support for the Azure Functions HTTP trigger for Java by Glenn Gailey, Christopher McClister, Daniel Oh, Carolyn McSharry,...</content><dc:creator>James Cobb</dc:creator></entry><entry><title type="html">Serverless Workflow integration with Camel Routes</title><link rel="alternate" href="https://blog.kie.org/2023/02/serverless-workflow-integration-camel-routes.html" /><author><name>Ricardo Zanini</name></author><id>https://blog.kie.org/2023/02/serverless-workflow-integration-camel-routes.html</id><updated>2023-02-10T17:58:06Z</updated><content type="html">A recent addition to the Kogito Serverless Workflow is the ability to call ! WHEN TO USE CAMEL ROUTES that provides to integrate with virtually any technology stack. Kogito Serverless Workflow offers a few ways to make calls to remote services that expose standard interfaces such as OpenAPI and AsyncAPI. Sometimes, you might need to make calls to legacy systems or specific services that require additional configuration, schema, or data structures so that the standard interfaces might not suffice. For example, you might need to call a from the workflow. Without additional Java implementation in your Kogito workflow application, you won’t be able to make requests to this SOAP service directly. What if you could explicitly declare in your workflow definition that your function is a call to a Camel route in your project context? HOW THE INTEGRATION WORKS Kogito Serverless Workflow has a new add-on capable of producing messages to a Camel Route within the same application instance. For example, you have a Camel route in your project that can interface with a SOAP WebService like this one: &lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;!-- might look like that we are using Spring, but we are not. The XML DSL is derived from there, though --&gt; &lt;routes xmlns="http://camel.apache.org/schema/spring"&gt; &lt;route id="sendSoapMessage"&gt; &lt;from uri="direct:numberToWords"/&gt; &lt;!-- Converting our body to the type expected by the SOAP interface --&gt; &lt;bean beanType="java.math.BigInteger" method="valueOf"/&gt; &lt;setHeader name="operationName"&gt; &lt;constant&gt;NumberToWords&lt;/constant&gt; &lt;/setHeader&gt; &lt;toD uri="cxf://example.com?serviceClass=com.dataaccess.webservicesserver.NumberConversionSoapType&amp;#38;wsdlURL=/wsdl/numberconversion.wsdl"/&gt; &lt;/route&gt; &lt;/routes&gt; Then, from the workflow definition, you can declare a function that produces messages to this route: { "functions": [ { "name": "callSoap", "type": "custom", "operation": "camel:direct:numberToWords" } ] } Notice the new custom function with a new operation type. The operation is a URI scheme composed of the constant "camel:", the "direct:" endpoint, and its name. Kogito Serverless Workflow only supports producing messages to a endpoint at this time. To use this function in the State definition, you can refer to the function as you usually would: { "states": [ { "name": "start", "type": "operation", "actions": [ { "functionRef": { "refName": "callSoap", "arguments": { "body": "${ .number }", "headers": { "header1": "value1", "header2": "value2"} } } } ], "end": true } ]} The function arguments can have optional attributes, "body" and "headers." These arguments will be constructed as part of the handled internally by the Kogito engine. The body can be any valid JSON object and the headers must be a key/value pair. Your route is responsible for properly handling the message. In the example above, the body is the number contained in the JSON payload. The route response must be a valid Java bean object that can be serialized to JSON or a primitive type. Note that a JSON string is a valid output. The data will be merged into the workflow context data in the response attribute, for example: { "fruit": "orange", "response": { "number": 10 } } , you will find more information about this scenario and the complete project example. FINAL THOUGHTS The Camel Kogito add-on complements our work of integrating with the. In the use case described in this post, the Camel route is tightly coupled to the workflow. If you need to reuse the route or have more complex interface interactions (such as a REST endpoint), you should use Camel-K and interact with the services via OpenAPI interfaces. Overall, this new feature introduced by Kogito Serverless Workflow can solve many use cases and enable integration with any service interface or data format supported by Camel. It’s a new way of interacting with services that don’t have standard interfaces or formats available, all within the same application. In a world where many companies are looking to modernize their architecture and lift to the cloud, we believe this new feature can help them through the journey. Leave a comment or open a thread on our if you have any questions. The post appeared first on .</content><dc:creator>Ricardo Zanini</dc:creator></entry></feed>
